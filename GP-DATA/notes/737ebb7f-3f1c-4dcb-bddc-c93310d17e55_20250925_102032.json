{
  "note_id": "737ebb7f-3f1c-4dcb-bddc-c93310d17e55",
  "title": "rag implementation for documents",
  "content": "Transcript\n\n\nIntroduction to RAG Tutorial\nEveryone's talking about RAG. If you feel left out, this is the only video you need to watch to catch up. In this\nvideo, we'll learn Rag in a super simplified manner with visualizations that will make it easy for anyone to\nunderstand. No background knowledge in AI or AI models or coding or programming required. We'll start with the simplest\nexplanation of rag there is. Then we'll look into when to and when not to rag. We'll then look into what is rag. We'll\nthen understand some of the prerequisites such as vector search versus semantic search, embedding models, vector DB, chunking using a\nsimple use case, and finally bring all of that together into rack architecture. We'll then look into caching,\nmonitoring, and error handling techniques, and close with exploring a brief setup of deploying rack in\nproduction. But that's not all. This is not just a theory course. We have hands-on labs after each lecture that\nwill help you practice what you learned. Our labs open up instantly right in the browser. So there is no need to spend\ntime setting up an environment. These labs are staged with challenges that will help you think and learn by doing\nand it comes absolutely free with this course. I'll let you know how to go about the labs when we hit our first lab\nsession. For now, let's start with the first topic. Let's start with the simplest explanation of rag. Say you\nSimplest RAG Explanation\nwere to ask Chad WT what's the reimbursement policy for home office setup. You already know when you ask\nthis question that Chad GBT is going to give an incorrect answer because it doesn't have access to our policy\ndocument that's private to our company. So an LLM like GPT would hallucinate and\nprovide an incorrect or generic answer that's common to most companies. The problem here is that it doesn't have the\nnecessary context of what you're asking about. So what do you do? You look up\nyour internal policy document and get the section of the policy that describes home office setup by yourself. Then you\nadd that to your prompt and tell Chad GBT to refer to this policy. Now with\nthis additional information, Chad Gibbt is able to generate more accurate\nresponses and that is the simplest explanation of rack that stands for retrieval augmented\ngeneration. The part where you look up your internal policy documents and retrieve the relevant information is\nknown as retrieval. The part where you improve or augment your prompt with the\nretrieved information is known as augmenting. And the part where LLM generates a response based on the\naugmented prompt is known as generation. And that is something you've done unknowingly many times. Now, of course,\nthat is a very simplified explanation of rag. And when we talk about rag systems, that is not what we typically refer to.\nSo let's see what that is next. Now, you don't want your users to have to locate\nand retrieve relevant information by themselves. Instead, you want your users to simply ask the question, what's the\nreimbursement policy of home office setup? And our system that's based on rag should be able to do the lookup and\nretrieval of relevant information, improve or augment the user's prompt and\nget an LLM to generate the right response. Now, how exactly do we retrieve relevant information? How do we\naugment and how do we generate? And that's what we're going to discuss throughout the rest of this video. Now,\none of the common mistakes people make is to consider rag as the solution for everything. Rack is not the solution to\nWhen not to RAG?\nall problems. At the end of the day, we're all trying to get AI to generate better responses and there are different\nways to do that. We can prompt better. That's called prompt engineering. We can fine-tune models. And then there's rag.\nWhen to use what? Let's take a simple use case to understand these better. So, back to our use case. We've started to\nnotice a lot of people copy pasting company policies into chat GBT to get answers. So we decided to build an\ninternal chatbot that can answer people's questions. We call it the policy copilot. It is a system that\nusers can simply ask a question such as what's the reimbursement policy and our chatbot system should be able to locate\nthe necessary information from the internal policy documents and then generate accurate responses and send\nthat back to the user. Now we also want to add some restrictions and limitations. We don't want the chatbot\nto answer everything. Some questions should be off limits like performance review appeals or salary discussions.\nAnd when those topics come up, we want to direct users to HR directly instead of giving them answers in the chat. We\nalso want our chatbot to have a specific voice and style. So our CEO has this\nwarm Scottish accent and a particular way of speaking that makes people feel certain way. We want our policy co-pilot\nto sound just like that, authoritative and distinctly Scottish. So when the\nusers ask what's the reimbursement policy for home office setup, it responds when the users ask how many\nsick days do I get per year? It says when the user asks can I work from home\npermanently? It says and when the users ask when are performance reviews conducted it\nresponds as you can see it's not just the Scottish accent there's this what should\nI say refreshing candandor that tells it like it is let's look at how to solve each of these areas the restrictions and\nsecurity requires us to define how the chatbot responds what it must reveal and what it must not so these are strict\ninstructions provided to the LLM to control its behavior based on users request such is never to reveal personal\nemployee information or confidential details. If someone asks about sensitive topics, politely redirect them to HR.\nPrompt engineering best practices are a good solution to this. Think of it as the rule book that keeps our chatbot\nsafe and professional. Next, we look at how to solve the problem of voice, style, and language. Now, we know if we\nasked Chad GBT to simply respond to me in a Scottish accent, it would. But the accent, as we saw earlier, is not simply\nwhat we are going after here. We wanted to speak like our Scottish CEO, use words he usually uses, the tone, the\nlanguage. So, we take all of his past speeches, he's given, emails written by him, blog post, videos created, and\nfine-tune a new model that can respond in the same language and tone. A good solution for this is fine-tuning.\nFine-tuning is the process where you provide a model hundreds of sample questions and sample answers and have it\nrespond to you in that way all the time. Now, you might be wondering, why can't\nfine-tuning solve this information problem? Why can't we train a model with all of the questions a user might ask\nand answers it can generate? The problems are the policies can change constantly and when they do, you need to\nretrain the model every time and trainings are not easy. They're expensive and slow. Retraining takes\ntime and computational resources. Users can't verify where the answers came from, so there's no citations possible.\nThe larger the training data, the lower the accuracy. And then there's knowledge cutoff. The model only knows what was in\nthe training data. Fine-tuning is great for stable unchanging patterns like communication style, but terrible for\ndynamic factual information. And finally, the best solution to get the most accurate responses is rag. Rag\nworks because it retrieves information dynamically at query time, not at training time because the whole point of\nrag is retrieving the most relevant information for the user's query real time. Next, we'll look at rag in more\ndetail. Let's now look at what rag is in the first place. So far, we've decided\nWhat is RAG?\nthat we're going to build our policy copilot system where employees can ask question and it retrieves the relevant\ninformation, augments prompts, and generates a response. We'll now see how each of these work. Let's look at\nretrieval first. Retrieval is a process of retrieving relevant information. But how do you do that? There may be\nhundreds of policy documents. How do you find which one is the right one that has context related to the user's question?\nAnd what do you search for within these files? First, we identify a few keywords from the user's question. In this case,\nwe've identified reimbursement and home office to be the relevant keywords. One of the simplest ways is to use a GREP\ncommand to search for specific terms in these files such as reimbursement or home office and hope that one of these\nfiles will have these terms. Alternatively, if these files were stored in a database, you could run a\nquery against it like this. Now, these would only return content that exactly matches the keywords we are looking for\nand the chances of getting accurate information every time is low. This approach of searching the documents with\nthe exact words is known as keyword search and it is a very popular technique that's used by many of the\nsearch platforms. To explain it simply, this approach goes through all the documents, identifies keywords and ranks\nthem based on their frequency. In this case, it counts the occurrences of reimbursement in all documents and\nrecords them. So we have three occurrences in the first document, none in the middle two, but another three in\nthe third one. It then does the same for home office and we see that it's only present in the home office setup\ndocument. Combining these two columns is now able to identify the document that\nhas the maximum occurrences of these two keywords and thus able to rightly select the document that has these keywords.\nNow that was a super simplified explanation. Keyword search is a science in itself and has a lot of complex\ncalculations that go in and there are multiple proven approaches available. Two of the most popular techniques used\nare known as TF and BM25. We won't go into the specifics of how these work.\nWe'll just see how to work with them. Let's see each of these in action. First, we import the TF vectorzer from\nthe scikitlearn open-source Python library. Think of the scikit library as a toolbox with pre-built algorithms that\nyou can use without having to write them from scratch. We then define three sample documents. The documents are\nsimple sentences for now. you could read the contents of a file in. We then create the TF ID of analyzer and we'll\ncall that analyzer. The word scores can then be calculated by running the fit transform method. We then print the\nresults on screen. The word scores show a bi-dimensional array with the importance of each word in each\nsentence. The word office appears in all sentences. So they get a score of 0.4. The first sentence identifies words,\nequipment, and policy and gives them a score of 0.7 and 0.5. The second sentence identifies the words furniture\nand guidelines and the third identifies the words travel and policy. Now that the vectors are created, we run a query.\nWe use the analyzer to transform to query the word furniture. What it does is it returns an array that returns a\nscore that compares the query word furniture to each document. Now let's see the same with the BM25 techniques.\nWe use the rank BM25 library which is a popular library that implements a BM25 algorithm. We then create what is known\nas the BM25 index and then get the word scores. In this case, we can see some differences. The word office gets a\nscore of zero because the BM25 algorithm is a bit more strict in assigning scores. And because this word is present\nin all documents, it doesn't see it to be very relevant. It then continues to assign a score for the most important\nand unique words in sentences like equipment in the first sentence, furniture and guidelines in the second,\nand travel in the third. And as before, we run a query, but this time using the get scores method and print the array.\nWe can see it's again identified the second document to be the relevant document here the right way. Well, it's\ntime to gain some hands-on practice on what we just learned. Follow the link in the description below\nFree Lab 1: Keyword Search (TF-IDF & BM25)\nto gain free access to the labs associated with this course. Create a free account and click on enroll to\nstart the labs. On the left side of the screen, you will see the list of labs. Only start the lab when I ask you to.\nWe'll do only one lab at a time. Let's start with the first lab. Click on start to launch the lab. Give it a few seconds\nto load. Once loaded, familiarize yourself with the lab environment. On the left hand side, you have a questions\nportal that gives you the task to do. On the right hand side, you have a VS Code editor and terminal to the system.\nRemember that this lab gives you access to a real Linux system. Click on okay to proceed to the first task. The first\ntask requires you to explore the document collection. Open the TechCorp documents in the VS Code editor. On the\nright, we see there is a TechCorp docs folder. Expand it to reveal the subfolders. The ask is to count how many\ndocuments are in the employee handbook. This is what I call a warm-up question that will help you explore and\nfamiliarize yourself with the lab. The real tasks are coming up. In this case, it's three, so I select three as the\nanswer. Then proceed to the next task. This is about performing a basic GP search. As we discussed in the lecture,\nwe'll run a GP command to search for anything related to holiday in the folder and store the results in a file\nnamed extracted content. To open the terminal, click anywhere in the panel below and select terminal. This creates\na new file with the results. Click check to check your work and continue to the next task. The next task is to set up a\nPython virtual environment and install dependencies. I'll let you do that yourself.\nWe'll move to the next task now. Here we explore the TF script. Here we first\nimport the TF vectorzer from the scikitle learn library. We then transform the dogs. Then we compare\nusing cosine similarities or cosine is one approach of comparing two vectors to identify similarities. And then we\nfinally print the results. We now execute the script and then we view the results. And for now we'll just click\ncheck to proceed to the next step. We then move to the next step. Here the question is to analyze the score printed\nand identify the score of the top results. So the ask is to search for pet policy docs and identify the score for\nthe top result. Here we see the top result is rightly identified as the pet\npolicy.md file with a score of 0.4676 whereas the other files have a score\nless than 0.1. So the answer to this question is 0.4676.\nThe next task is to review and execute the BM25 script. Open the BM25 search.py\nfile and inspect it. You'll see that we import the rank BM25 uh package. We then\ncreate an index and then for each query called the BM25.get scores method and from the results we get the top three\nresults and we go through each result and print that. Finally, there is a hybrid approach that combines TF and\nBM25 techniques using a weighted approach. I'll let you explore that by yourself. Let's get back to the next\ntopic. We just looked at vector search. Let's now understand semantic search.\nWhat are Semantic Search?\nNow, one of the challenge with keyword search is that if the exact keyword isn't there, the search fails. For\nexample, instead of reimbursement, if we say allowance, it tries to find the exact word allowance. And instead of\nhome office, if the user asks work from home, it's unable to find that anywhere.\nThese combination of keywords aren't found in the documents and thus the document is not found. In our example\ncode, say we say desk instead of furniture is not going to be able to find any matches in scores and thus\nunable to find any matching document. That's the limitation of keyword search and that's where we need semantic\nsearch. Semantic search searches documents based on the meaning of words and thus have higher chances of locating\nthe right documents based on the inputs and that's what we will look at next. So what exactly is semantic search? Think\nof it as search that understands meaning not just words. When you search for allowance, semantic search can find\ndocuments about allowance or reimbursements or anything that has similar meaning even if those exact\nwords aren't used. Similarly, if you search for home office or work from home, it can find documents that has\nanything to do with remote work. The magic happens through something called embeddings. We convert both your search\nquery and all the documents into mathematical vectors. Think of them as coordinates in a highdimensional space.\nDocuments with similar meanings end up close together in this space. So, when you search, we find the closest matches\nbased on the meaning, not just word overlap. We can measure how similar two pieces of text are by calculating the\ndistance between their vectors. The closer the vectors, the more similar the meaning. So reimbursement and allowance\nwould have vectors that are close together even though they're different words. We'll see this in more detail\nnext. Let's now understand embedding models. So if you look at machine learning models, they can be categorized\nUnderstanding Embedding Models\nat a high level based on use case such as computer vision, NLPs or natural language processing, audio among many\nothers. And within each category, you have a number of models available. This\nis as shown on hugging face which is a popular platform where you can discover models, data sets and applications. Our\ninterest here is the sentence similarity within natural language processing. And within sentence similarity, one of\nthe popular models is sentence transformers all mini LM L6V2.\nThis model map sentences and paragraphs to a 384 dimensional dense vector space\nand can be used for clustering and semantic search. It is also a 22 million parameter model. Now, what does that\nmean? The parameter size reflects the brain power of the model. Think of parameters\nas the learned knowledge stored in the AI's memory. Each parameter is a number\nthat the model learned during training to understand language patterns. 22 million parameters means this model has\n22 million learned values that help it understand how words relate to each other, what sentences mean semantically,\nwhich concepts are similar or different. Let's compare that to things we already know like GPT models. Let's compare this\nmodel to GPT 3.5 and GPD4 model that we use. The 22 million parameter size of\nour all mini LLM model is very small compared to the 175 billion parameters\nof GPD 3.5 and 1.8 trillion parameter size of GPD4. The size of the model is\nproportional to that too. The all mini model is 90 mgabytes in size. As such,\nit can be used locally in our laptops and the size of GPT 3.5 and 4 are 350GB\nand 3.6TB respectively. and thus the use case differs. The all mini LM model is a\nperfect fit as an embedding model for our use case whereas the GBT models are used for text generation and reasoning.\nEmbeddings and Vectors\nSo we just mentioned embeddings. What are they actually? As its simplest form, an embedding model takes text and\nconverts it into numbers that represent meaning. So sentence like dogs are\nallowed in the office is converted into an array of numbers known as vectors.\nWhen you give the model a sentence like dogs are allowed in the office, it doesn't just look at the words. Instead,\nit thinks about what this sentence actually means. Is it about animals? Is it about workplace policies? Is it about\npermissions? The model then creates a list of numbers that captures all these different aspects of meanings.\nEach number represents something that model learned about language. Maybe the first number captures how animal related\nthe text is. The second number captures how workspace related it is and so on.\nAnd it then plots that in a graph. So dogs get a number 0.00005597\nand is added to a section of the graph that represents animals and pets also\nfall into the same category. However, remote does not go there. Similarly,\noffice falls into the workplace area. So, our first sentence moves closer into\nthe workplace section and so does the second sentence because it is also related to work. And the same applies to\nthe last sentence as that's also related to the workplace. We then compute the distance between these points. The\nshorter the distance, the closer they match. So, finally, if you look at these two sentences, you'll see that the first\ntwo are similar. That's a similarity search explained in the most simplest of forms. And this explanation only works\nfor a two-dimensional array. But in most cases the dimensions are too many that we can't even imagine how it would look\nvisually. In this case the model we are using uses 384 dimensions. So we don't\neven know how to imagine this or plot that on a graph. So then how do we calculate similarities between them?\nThis is where the magic of mathematics comes in. Since we can't visualize 384 dimensions, we need a mathematical way\nThe Dot Product\nto measure how close two points are in this highdimensional space. The solution is something called the dot product.\nThink of it as a mathematical ruler that can measure distance in any number of dimensions, even ones we can't see. So\nhere's how it works in simple terms. For the sake of simplicity, I'll convert the vectors for each sentence into\ntwo-dimensional vectors of simple numbers. So dogs are allowed in the office gets a vector value of 1, 5. The\nsecond sentence gets 2 and four and the third one gets 6 and 1. The process\ninvolves multiplying the vectors, adding them, and then normalizing them. Let's\nlook at the first two. We first multiply the values in the vectors. So, we multiply 1 * 2 to get 2 and 5 * 4 to get\n20. We do the same for the other two pairs. We multiply 1 5 with 6 and 1 to get 6 5.\nAnd then we multiply 2A 4 with 6 and 1 to get 12A 4. We then add the multiplied\nnumbers together. So 2 + 20 gives us 22. And we get 30 and 16 for the others. And\nfinally, these go through a normalization process to convert these numbers into anything between 0 and 1.\nThat also takes into consideration the total size of the vectors among other things. Finally, the pair with the value\nclosest to one are similar and away from one are dissimilar. So that's a basic\nexplanation of how sentences are compared for similarity. Now, of course, you don't have to do all\nof that math by yourself. We have libraries that does that for you. Numpy is a powerful Python library for working\nwith numbers and mathematical operations. We import numpy as np and\nthen we call the np dot method and pass in the vectors for it to calculate the dotproduct between the two vectors. It\nreturns a similarity score between zero and one. So let's take a closer look at that. So\nfirst we install the required libraries such as the sentence transformers and numpy library. So the sentence\ntransformers as we saw provides the sentence transformer class and the all mini lm model. The numpy provides the np\nfunction for calculating dotproducts between vectors. So here we can see the complete code in action. We first import\nthe sentence transformers library and the numpy library. Then we load the all\nmini LM L6V2 model that we've been discussing. What it does is it downloads\nthe model, loads the 22 million parameters into memory, prepares a model to convert text into embeddings. We then\ndefine our three test sentences about dogs, pets, and remote work. And we then\nencode these sentences into embeddings using the embedding model. And finally,\nwe calculate the similarity between each pair of sentences using NumPai's dotproduct function. Now, let's see what\nhappens when we run this code. We print out the similarity scores between each pair of sentences. And the results are\nquite interesting. Looking at the results, dogs versus pets shows 73.3%\nsimilarity. That makes sense because both are talking about animals in the workplace. Dogs versus remote shows only\n36.2% similarity. That makes sense, too, because one is about animals, the other is about work arrangements. Pets versus\nremote shows 33.8% similarity. Again, these are quite different topics. This demonstrates exactly what we've been\ntalking about. The model can understand semantic meaning, not just word matching. Even though dogs and pets are\ndifferent words, the model recognizes they're both about animals in the workplace context. And it correctly\nidentifies that remote work policies are quite different from animal policies. And this is the foundation of how rack\nsystems are built. They can find semantically similar content even when the exact words don't match. This is\nwhat makes rag so powerful compared to traditional keyboard search. So, so far we've been looking at sentence\ntransformers and the all mini LM L6V2 model. But sentence transformers are just one example of embedding models.\nThere are many other popular embedding models out there that you can choose from depending on your use case. Now,\nlet me clarify an important distinction. The sentence transformers we've been using are local models. They run on your\nlocal machine. They're completely free and they don't require an internet connection. But there are also remote or\nAPI models like OpenAI's embeddings that run on external servers where you pay\nuse and need an internet connection. In this sample code, you can see how we use the OpenAI library and use the\nembeddings API endpoint to create a new embedding. The model is text embedding 3\nsmall and it returns the embedding vector for it. There's also this leaderboard of top embedding models\nposted by HuggingFace. We can see some of the most popular ones here. Gemini topping the chart with Quen 3 and others\nthat are following. Well, that's all for now. Head over to the labs and practice working with embedding models. All\nLab 2: Embedding Models\nright, we're now going to look into the second lab. This is called embedding models. So, I'm just going to click on\nstart lab to start the lab. We'll give it a few minutes to load. Okay, so in this lab, we're going to look at uh\nembedding models. We'll explore semantic search using embedded models which are the foundation of modern uh rag systems.\nSo let's uh go to the first task. So the first task is about keyword uh search\nlimitations. So first we navigate to the project. We create a new virtual environment and install the requirements.\nI go to the terminal and we're going to uh set up the virtual environment.\nSo our project is within this uh folder called rag project. And here we have the\nvirtual environment uh that's being set up. Okay. Okay, we now run the um the next\nstep once the once the virtual environment is set up, the next step is to run the keyword limitation demo. If\nyou go to the rag project, you'll see the keyword limitation uh demo script. So, this is a simple script that\nsearches for a word or keyword that does not exist in in the documents and proves\nthat uh pure keyword based uh search uh are less likely to yield the right\nresults. For example, in this case, the query is distributed workforce policies and the none of the documents have\nsomething that's exactly like that, right? So, let's try running the script.\nAnd if you look at the script, most of the scores are zero because um the keywords distributed workforce policies\ndoes not really exist in any of the scripts. So, the correct answer here is\nmissing synonyms and context. All right.\nThe next task is to install embedding dependencies. So we go to the rack project. So we're already in that project.\nWe source the uh virtual environment. We install the embedding packages. So I'm going to copy copy this command install\nit. So the packages are sentence transformers hacking phase hub and open AI.\nThe next question is to run the local embedding scripts. So if you see the script name is semantic search demo. So\nlet's look at the semantic search uh demo script. And if you look into this, we can see that the first step is loading the documents. And then we load\nthe local embedding model which is the all mini LML L6V2. And then we generate embeddings by calling the method model.\nAnd then we pass in the docs. And then we have the query which is the same query we used before which is distributed workflows policies. And then\nwe generate embeddings for the query. And then we calculate u the similarities using the np uh method. And then we\nprint the results. So let's uh run the script. Twitter uv run python semantic\nsearch demo. Now as you can see in the same set of\ndocuments the script has now identified the relevant uh documents uh which has\nthe meaning that's closer to the distributed workforce policies query that we are looking for. So if you see that for each document is uh given a\nrating and that means that it's able to identify the document that has the closest semantic results. We'll go to\nthe next question. So the task is to uh look at the results\nand then say look at the semantic search results and what is the similarity score between remote work policy and distributed work policies. If you look\nat the first score say 0.3982 and that is the score for remote work policy.\nThe next question is a multiple choice question. So this basically confirms our learning. So the question is based on\nthe comparison between semantic search and keyboard search which is a TF IDF and BM25 that we saw earlier. Which\napproach better understands the meaning of queries? Of course we know that semantic search understands uh the meaning of queries better. And that's\nbasically about uh this lab. In the next lab we'll explore um vector databases. Let's now understand vector databases.\nVector Databases Explained\nSo far we saw how we could use the sentence transformer libraries and load simple sentences into it to create\nembeddings and then compare those embeddings to each other in a super simple way. However, we have a bigger\ntask at hand. Our policy co-pilot system and it has hundreds or thousands of large policy documents. Let's say we\nhave 500 policy documents each with multiple sections. When a user asks, \"What's the reimbursement policy for\nhome office setup? Our system needs to search through all of these documents to find the most relevant ones.\" Now, if we\nwere to do this the naive way, comparing the query embedding with every single stored embedding, we'd have a big\nproblem because with 500 documents, each with 384 dimensions, that's 192,000\ncalculations for every single query. This is like searching through a phone book page by page. It works for a small\nphone book, but imagine trying to find a specific number in a phone book with millions of entries. You'd be there all\nday. That's where vector databases comes in. Think of them as having a smart librarian who knows exactly where to\nlook. Vector databases can retrieve relevant results instantly. They efficiently use resources. They're\nscalable and they do that by using smart indexing algorithms. What does indexing\nmean? Earlier we saw how we represented documents or sentences on a vector graph\nand then compared their similarities. But when there are thousands of such policies, it's going to be impossible to\ncompare them. And that's where indexing comes in. Instead of checking every single vector, we pre-organize them into\nneighborhoods. In this case, the animal policies are grouped together. All health benefits are grouped together.\nAll remote work policies are grouped together. That way, when someone asks about bringing their dog to work, we\ndon't search the entire space. we go directly to the animal policies neighborhood and only search there.\nLet's look at the three most popular indexing algorithms used by vector databases. HNSW or hierarchical\nnavigable small world is the most widely used algorithm. It creates a graph\nstructure where each vector is connected to its most similar neighbors. So when searching, it starts from a random point\nand follows the connections to find the closest matches. It's fast and accurate, which is why most vector databases use\nit by default. IVF or inverted file and LSH or locally sensitive hashing are\nother examples of the same. Let's now look at some of the popular vector DB implementations. Chroma is perfect for\nlearning because it's open- source and Python friendly. We can install it on your computer and start experimenting\nimmediately. It's free, which makes it great for students and small projects. Pine cone is a managed service, meaning\nthey handle all the infrastructure for you. You just send your data and queries and they take care of everything else.\nIt's used by big companies in production, but you pay per use. There are other great options too. VV8 with\nits GraphQL API is another example. But for learning, I recommend starting with Chroma. So the best approach is to start\nwith Chroma for learning and experimentation and then move up to Pine Cone or similar services for production\nuse case. So first we install the required library such as the Chroma database. Then we\nChromaDB Tutorial\nimport the Chroma DBA library. We connect to the client. We create a collection called policies. Chroma\ncreates a new collection in memory. Sets up the default embedding model. The all mini LM embedding model. Prepares\nstorage for vectors and metadata. We then add policy documents to the collection using the collection add\ncommand. So this converts text to 384dimensional vector that we spoke\nabout earlier. Saves the vector in the collection, adds the vector to the hnsw\nindex structure. The document is immediately searchable. To search, we run the collection.query method and pass\nthe query string. Now let's talk about some important Chromb concepts. First, the default behavior of Chromadb is that\nit's not persistent. When you create a client with just chromadb.client, client, it stores everything in memory.\nThis means when your program stops, all your data is lost. This is fine for learning and experimentation but not for\nproduction. To make Chromb persistent, you need to use persistent client instead of client. You specify a path\nwhere you want to store the database files. This way, your data survives program restarts and you can build up\nyour vector database over time. You can also change the embedding model that Chromma DB uses. By default, it uses the\nall mini LM model, but you might want to use a different model for better performance or to match what you used\nduring training. You can use OpenAI's embedding models or even create a custom embedding function using any model you\nwant. In this case, we pass in a new parameter called embedding function that passes in OpenAI's embedding function as\na parameter along with the API key. Let's head over to the labs and gain hands-on experience.\nLab 3: Vector Databases\nOkay, let's now look at the uh lab on vector DB. So I'm going to start the lab\nnow. Okay. So the lab has uh what I'm going to do is I'm going I'm just going to go through a high level overview of\nthe lab and I'll leave you to do most of it but I'll just explain how the lab functions. Right. So uh in this lab\nwe're going to learn how to scale the semantic search with vector databases. So let's get that going.\nSo the first task is to simply understand the uh concepts. So before we\nstart building, let's uh understand what vector databases are. So we already discussed that in the video, but here's\na quick uh description of what it is and what it can help us do. And there's a question on um what is the primary\nadvantage of using a vector database or strong embedding models um in memory. So\nI'll uh let you answer that uh yourself. The next step is to navigate to the project directory which is right here.\nAnd then um we again activate the virtual environment and we install the embedding uh model package which is\nsentence transformers which we also did in the last lab. And then the next step is to install the vector database. In\nthis case we're going to use chromb. So um the task is to install the chromb package.\nAgain I'll just skip through that for now. Uh the next task is to initialize a chromb vector database. So um if you go\nhere there's a script called init vector DB and if you look into the script we first import the chromb package. We also\nhave the sentence transformers. Um we then uh create uh the chromb client\nusing the chromadb.client method. And then we create a collection. We'll call it techcorp docs. And then uh we load\nthe embedding model which is all mini LM L6 model. And then we um\ntest the model with a sample document. So we have identified a test doc which\nis really just a sentence that's given here. Um we'll then add the test\ndocument to the collection using the collection add method and we'll print the results and then uh we'll print the\ncount of uh documents within the collection. And that's basically it. So that's a a quick uh beginner level uh\nscript. In the next one, there are a couple of questions that are being asked. So uh\nyou can answer those questions based on the results of the script. The next one is uh called as store documents. So this\nis where we store actual documents within the chromob uh database. Again, this is another script that starts off\nand loads the model and client as we did before. uh but in this case we're reading the tech corp docs um documents\nusing the tech corp docs method which we have in the utilities function. So that's what loads all the uh documents\nthat are in the tech corp uh docs folder. So now we're loading actual documents\nand then we follow the same approach of adding those documents to the collection and then we verify the collection. So\nagain just uh another uh layer to that uh script the basic script. In this case\nwe're just storing documents. We'll continue to the next task. This is where we do perform uh a vector search\nagainst the documents. So um the script this time is vector search uh demo. So\nclick on the vector search demo script. And here we have uh some sample documents um there are sentences and\nthen there's a query. Let's now understand chunking. Now that we understood how vector databases work, we\nChunking Explained\nhave a new challenge. We've been working with simple sentences like dogs are allowed in the office on Fridays. But\nwhat happens when we have real policy documents? What if we have a 50page employee handbook that we want to add to\nour vector database? Let's think about this practically. We have an employee handbook, 50 pages of policy content,\nmultiple sections per page, complex policies with detailed explanations.\nWhat happens when we try to add this entire document to Chromob as a single entry? Well, it would work. Technically,\nChromob would create an embedding for the entire document, but when someone asks what's the remote work policy,\nthey'd get back with the entire 50page handbook. That's not very helpful.\nThis is what I call the precision problem. Without chunking, when someone asks what's the remote work policy, they\nget the entire 50page handbook. The user gets overwhelmed with irrelevant information. They have to search through\neverything to find what they actually need. But with chunking, we break that\nhandbook into smaller focused pieces. Now, when someone asks about remote work, they get back just the specific\npolicy sections that are relevant. The user gets exactly what they asked for. clear focused answers. Now, how do we\nDocument Chunking Strategies\nactually break documents into chunks? There are several strategies, but we'll focus on some of the simplest ones. With\nfixed size chunks, we simply take 500 characters per chunk. This is simple and\nreliable for most use cases. We just split the document into equals sized pieces, which makes it easy to\nunderstand and implement. But there's a problem with this approach. What happens when we split\nright in the middle of a sentence? We might end up with dogs are allowed in one chunk and on Fridays in the other.\nThis breaks the meaning and makes it hard for the system to understand the complete information. That's where\noverlap comes in. We add a 50 character overlap between the chunks. So the end\nof one chunk overlaps with the beginning of the next. This way if we do split the sentence, the important context is\npreserved in both chunks. Now there are other methods of chunking\nlike sentencebased chunking. This is where every sentence is split into a\nseparate chunk or paragraph based chunking where each paragraph becomes a\nsingle chunk. Chunking might sound simple but it's actually quite tricky. The main challenge is finding the right\nbalance. If chunks are too small we lose context. So as we saw earlier, if one chunk has docs are allowed and on the\nother chunk has on Fridays, the user would get incomplete information. We'd have poor understanding because we're\nmissing important details and the information would be fragmented. On the other hand, if chunks are too\nlarge, we have poor precision. If we put an entire policy in one chunk, we're back to the same problem we started\nwith. The search would be inefficient because there's too much irrelevant content and the results would be\noverwhelming. So it's important to choose the right strategy based on your requirements. Apart from the fixed size\nchunking, there are other methods like sentencebased chunking and paragraph that we saw, but even others like semantic chunking and agentic chunking\nthat is for now out out of scopes of this video. Now let's build a simple\nchunking function. This function takes a document and splits it into overlapping chunks. The key features are it tries to\nbreak at sentence boundaries when possible. It maintains the overlap for context and it handles the end of the\ndocument properly. Now this is a simple chunking done by a Python library. Now let's see how chunking integrates with\nour vector database. The complete workflow is that we chunk our large policy document, add each chunks to the\nvector database with a unique ID and then when we query we get back with the specific chunks that are most relevant.\nThis gives us the best of both worlds. We can handle large documents, but we get precise, relevant answers. Instead\nof searching through entire documents, we are searching through focused chunks that contain exactly what the user is\nlooking for. Let me share some key principles for effective chunking. For size guidelines, 200 to 500 characters\nis a good balance of context and position. With 50 to 100 characters overlap to maintain continuity, you\nmight need to adjust based on your content. Technical documents might need different chunk sizes than general policies. For boundary rules, always try\nto split at sentences to maintain grammatical integrity. Avoid midword breaks to keep words intact and preserve\nparagraphs to maintain logical structure. Finally, always test the real queries to ensure your chunks actually\nanswer questions. Verify that the overlap reserves meaning and monitor your search results to see if you need\nto adjust the chunk size. Remember, chunking is all about finding the right balance between context and precision.\nIt's not just about breaking documents into pieces. It's about breaking them in a way that makes sense for your users.\nAll right, let's look into the next lab on document chunking. Okay, in this lab,\nLab 4: Document Chunking\nwe're going to look at uh chunking techniques. So, we'll learn how to optimize rack performance by breaking\ndocuments into focused searchable chunks.\nSo, first we activate the virtual environment. So, this is something we have uh already done many times.\nAll right. Uh, so first we're going to look at this chunking problem demo script. So if you expand\nthe rack project, there should be a script called as chunking problem demo script. The thing is uh this script\ndemonstrates the core problem of searching a large documents in rack systems. It creates a sample employee\nhandbook and shows how searching for specific information like u internet speed requirements returns the entire\ndocument instead of just the relevant section. So uh we'll see uh a large document stored as a single chunk search\nqueries that should be that should find specific sections or results that return the entire document. So here you can see\nthere's a sample document um that has multiple sections and uh we're adding\nthat document to the uh collection chrom and then we're doing a query for\ninternet speed requirements. So let's run the script and see\nhow it works. So the script runs now. As you can see,\nit returns the entire document. It's truncated here, but uh the result shows the uh entire document. So that's the\nproblem uh with this uh approach. So the answer to this is large documents return irrelevant uh results. Next uh we will\nlook at some of the uh dependencies, libraries and dependencies that we'll be using. So first um we have what is known\nas lang chain. So if you uh don't know what lang chain is, we have other videos\nthat are on our platform. We have a future course that's coming up that will be for lang chain end to end. So do\nremember to subscribe to our channel to be notified when it comes out. So lang chain is a powerful framework for\nbuilding rag applications. It provides recursive character text splitter for smart uh document chunking and there's\nalso the uh spacy which is an advanced natural language processing library and it provides uh it also provides a spacey\ntext splitter for sentence aware chunking. So we'll use spacy for sentence um aware chunking and it uh\nthese libraries take care of chunk sizes overlaps operators etc. And we'll\ninstall the lang chain and spacey dependencies. Okay, we'll go to the next\nquestion and we'll first look at basic chunking. So if you open the basic uh chunking\nscript, you'll see that it uses the lang chain uh text splitter um from which we have the recursive\ncharacter text splitter uh library. So here we have a sample document and uh\nthis is where we are doing the splitting. So as you can see we specify the chunk size 200, the chunk overlap is\n50. So that's the uh 50 characters that's going to be overlap between each chunks or some of these se separators\nthat are defined. So we then do a splitter.split text to split the text into different chunks and\nthen we have we just go through the chunks and print them. So I'll let you do that yourself. We'll\ngo to the next one and there's uh a bunch of questions uh that are asked\nthat you can you have to read the script and understand and answer. So I'll let you do that uh by yourself.\nThe next one we'll look at is sentence chunking. So in sentence chunking again uh if you look at the script we're using\nspacy as a library. And then we have um a question that's based on the output of\nthat script. And then finally we looked at chunked search. So this is a another\nscript that performs a chunked vector search uh demo that kind of connects\neverything we have uh learned so far together. So first we chunk the documents and then we add these chunk\ndocuments to a collection uh and there's comparison between chunked no chunking a collection with no chunking and\ncollection with chunking and then uh we'll see the difference between the two. Again I'll let you u go through\nthat by yourself and there's a question that's based on that. So, yep, that's u a quick lab on chunking and I'll see you\nin the video. Let's now bring it all together to build our rack system. Now that we understand all the individual\ncomponents of racks, that's retrieval, augmentation, and generation. It's time to see how they all work together in a\nreal system. We've been building our policy copilot system piece by piece. But what does it look like when\neverything is connected and running in production? So, we know the basic flow. User query goes to retrieval, then\naugmentation, then generation, and finally response. But this is just the highle view. In a real system, there are\nmany more components working behind the scenes to make this happen smoothly, efficiently, and reliably. Now,\neverything we spoke about so far, such as chunking, creating embeddings, storing it in vector DB, etc. are things\nthat need to be done before the user starts asking questions. because loading\nthousands of documents, chunking and storing them in DB and creating embeddings out of them and scoring them\nall of that takes a lot of time and so they go together before this stage called as a rag pipeline. Let's take a\nBuild your RAG Architecture\ncloser look at that simple rag pipeline. The rag pipeline gets the policy\ndocuments, chunks them into small pieces using a chunk size of 500 with an\noverlap of 50 characters, then converts them into embeddings using OpenAI's\nembedding models and then finally loads them into a vector DB.\nNow, when a query comes in, we search the rag pipeline and it gives us the\nnecessary chunks of document. We then augment that document along with the\nuser's query and sends that to the LLM to generate a response. So that's a\nsuper simplistic rack pipeline. Let's head over to the labs and see this in action. All right. So this is the last\nLab 5: Complete RAG Pipeline\nlab in this course and this one is about building a complete rag pipeline. So uh\nwe'll learn how document chunking integrates with vector search, how query processing connects to retrieval, how\ncontext augmentation feeds into response generation, and how the complete rag pipeline works end to end. So this\nbasically combines everything from the first four labs that we've just done.\nAll right. So first we start with setting up the virtual environment. So the environment is already set up. You\njust need to activate it. All right. So, first we start by looking\nat the complete rack demo script. So, we have a single script now that combines\neverything we've done so far. And uh we'll start looking at it uh\nsection by section. So, there's the first section that has the document loading and chunking. And there's the a function for that. We have some sample\ndocuments. And then we have a text splitter. And we have all the uh chunks\nthat are created here. And then we have section two which is a vector database setup. Here you can see we set up a chromb vector database and store the\ndocument chunks there. And then we have the uh user query processing section.\nThis is where we actually process the user queries. And then we do the actual search. And finally we have the context\naugmentation. This is where we build augmented prompt with retrieved context for LLMs.\nAnd so here you can see how uh a prompt is generated with the uh context in\nplace which is the basically the policies that were retrieved and then you have the actual question the user's question itself and some additional uh\nprompt engineering and then finally we have the generate response function that generates a\nresponse using LLM and finally we have the complete rag\npipeline that calls each of those functions. funs that we have written\nbefore and then there's the main function. Well, I'll let you explore this uh lab\nby yourself. There's a lot of uh interesting questions and challenges throughout.\nCaching, Monitoring and Error Handling\nThis section covers the essential production concerns. Caching to make systems fast, monitoring to know what's\nhappening, and error handling to keep systems running when things go wrong. Let's start with a fundamental problem.\nRag systems are slow. Every query involves multiple expensive operations.\nGenerating embeddings, searching vector databases, calling LLM APIs. Without optimization, a single query can take\nnearly a second. But here's the thing. Most queries are repeated or are very\nsimilar. People ask the same questions over and over. What's the reimbursement policy for home office setup? Gets asked\ndozens of times. Caching solves this by storing the results of expensive\noperations and reusing them. Instead of taking 950 milliseconds, a cache\nresponse might then just take just 5 seconds. That's 190 times faster. The\nkey insight is that we don't need to recomputee everything for every query. We can cache at multiple levels, the\nembeddings, the search results, or even the final answers. So there are four main types of caching that we can\nimplement in rag systems. each solving a different performance bottleneck. Query cache is the simplest. We store complete\nquestion answer pairs. When someone asks what's the remote work policy, again, we return the exact same answer instantly.\nThis works great for frequently asked questions. Embedding cache stores the computed vectors for text. This is\nuseful because generating embeddings is expensive and we often process the same text multiple times like policy chunks\nthat appear in multiple searches. Vector search cache stores the results of database queries. This helps when\nsimilar queries return the same results. Remote work and working from home might return identical chunks. Llm response\ncache stores the generated answers. This is the most expensive operation to cache but also the most valuable since LLM\ncalls are typically the slowest part of the pipeline. The key is to cache at the\nright level, not too granular, not too broad, and with appropriate expiration times. Let's look at how to actually\nimplement caching. Well, Reddis is a popular caching tool because it's fast, supports different data types, and has\nbuilt-in expiration. The example shows a simple but effective caching strategy. We create a unique cache key by hashing\nthe query and context together. This ensures that different queries can get different cache entries, but similar\nqueries can share the same entry. We check the cache first. If we find a cache response, we return it\nimmediately. If not, we generate the response using our normal rack pipeline,\nthen store it in the cache with an expiration time. The TTL or time to live\nis crucial. We want to cache to we want to cache long enough to get performance\nbenefits, but not so long that the data becomes stale. For policy documents and R might be appropriate for more dynamic\ncontent, we might use shorter times. You can't manage what you don't measure. In\nproduction, we need to monitor everything to understand how our rag system is performing and when problems\noccur. The best metrics are response time, how fast we answer questions, throughput, how many queries we handle\nper second, error rate, what percentage of requests fail, but rack systems have\ntheir own specific metrics we need to track. Retrieval quality measures how relevant the return chunks are to the\nuser's question. Embedding performance tracks how long it takes to generate vectors. Chunking efficiencies monitors\nhow well we're breaking up documents. We set alerting thresholds to know immediately when something goes wrong.\nSo if response time exceeds 2 seconds, there's uh there's a performance issue. If error rate goes above 5%, then\nthere's a system problem. So the key is to set realistic thresholds based on\nactual performance, not theoretical targets. So we want alerts that indicate real problems, not false alarms that\ncause alert fatigue. Now things will go wrong in production. Vector databases will go down. Llm services will be\nunavailable. Networks will have timeouts and we need to handle these failures gracefully. The goal is graceful\ndegradation. The system should still work even if not at full capacity. So users should get some answer rather than\nan entire error message. So the example uh shows a cascading fallback strategy.\nIf the full rack pipeline fails, we try keyword search. If that fails, we return\nthe retrieved chunks directly. If even that fails, we use simple text matching.\nAnd as a last resort, we return a helpful error message. And we periodically test if the service is back\nby sending a few requests. And this is uh the halfopen state. So if those succeed, we close the circuit and resume\nnormal operation. Let's now bring it all together to build our rag system. Now that we understand the core rack\nRAG in Production\narchitecture, we need to talk about what happens when we put these systems into production. Real world rack systems face\nchallenges that don't exist in our simple examples. Performance issues, failures, and the need to handle\nthousands of users. So this diagram shows a complete production rack system running on Kubernetes. And let me walk\nyou through each layer. So we have a data layer, a rag pipeline layer, and the application layer, and a monitoring\nstack. So the data layer includes all our storage systems. So Chromad for vectors, Red is for caching, PostgresQL\nfor metadata. The rag pipeline layer contains the core rack functionality broken down into microservices. So query\nprocessing, chunking, embedding, generation, retrieval, augmentation and generation. And each service can scale\nindependently based on demand. The application layer contains all the userfacing services. So there's the web\nUI, there's the mobile app back end if there's any the admin interface etc. These services handle users interactions\nand present the rack capabilities through different interfaces and then we have our complete monitoring stack.\nPrometheus for metrics, graphana for dashboards, Jerger for tracing and the ELK stack for logging. Now this layered\narchitecture separates concerns clearly. Applications handle user interactions.\nThe rag pipeline processes the core functionality and the data layer provides storage. This can handle\nthousands of concurrent users while maintaining high availability and performance. Well, that's a highle\noverview. We haven't spoken about a lot of advanced topics like multimodal rack, graph rag, hybrid search techniques,\nfederated rack, reranking techniques, query expansion, context compression. To\nlearn more about AI and other uh related technologies, check out our AI learning path on CodeCloud. Well, thank you so\nmuch for watching. Do subscribe to our channel for more videos like this. Until next time, goodbye.",
  "keypoints": [
    "Transcript\n\n\nIntroduction to RAG Tutorial\nEveryone's talking about RAG",
    "If you feel left out, this is the only video you need to watch to catch up",
    "In this\nvideo, we'll learn Rag in a super simplified manner with visualizations that will make it easy for anyone to\nunderstand",
    "No background knowledge in AI or AI models or coding or programming required",
    "We'll start with the simplest\nexplanation of rag there is"
  ],
  "timestamp": "2025-09-25T10:20:32.809103",
  "created_by": "james-ui",
  "version": "1.0"
}